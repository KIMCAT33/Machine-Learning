{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ìì—°ì–´ì²˜ë¦¬ë¥¼ í•´ë³´ì",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KIMCAT33/Machine-Learning/blob/master/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%EB%A5%BC_%ED%95%B4%EB%B3%B4%EC%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UAdsu8MQljn",
        "colab_type": "text"
      },
      "source": [
        "<h2>ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•´ë³´ìğŸ¤“</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mGsfOFHQ4pt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h5>ì˜ˆì „ì— ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¥¼ ëª¨ë‘ ìˆ«ìë¡œ ë°”ê¿”ì•¼í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ë“¤ì—ˆë˜ ê²ƒì´ ê¸°ì–µë‚˜ì‹¤ê²ë‹ˆë‹¤.\n",
        "  \n",
        "  ì´ë¥¼ ìœ„í•´ ë‹¨ì–´ì™€ ë¬¸ìì˜ ì›-í•« ì¸ì½”ë”©ì„ í†µí•´ ë‹¨ì–´ë‚˜ ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¿”ë³´ê² ìŠµë‹ˆë‹¤.ì›-í•« ì¸ì½”ë”©ì€ í† í°[ë‹¨ì–´/ë¬¸ìì˜ ë‹¨ìœ„]ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê°€ì¥ ì¼ë°˜ì ì´ê³  ê¸°ë³¸ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ëª¨ë“  ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•˜ê³  ì´ ì •ìˆ˜ ì¸ë±ìŠ¤ ië¥¼ í¬ê¸°ê°€ N(ì „ì²´ ì–´íœ˜ ì¢…ë¥˜ì˜ ìˆ˜)ì¸ ì´ì§„ ë²¡í„°[ië²ˆì§¸ ì›ì†Œë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ì¸ ë²¡í„°]ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. </h5>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYgDap1ZQfFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "84deae86-57e3-4a89-ee34-01475a690e42"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# ì´ˆê¸° ë°ì´í„° : ê° ì›ì†Œê°€ ìƒ˜í”Œì…ë‹ˆë‹¤.\n",
        "# ì´ ì˜ˆì—ì„œëŠ” í•˜ë‚˜ì˜ ìƒ˜í”Œì´ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ì§€ë§Œ ë•Œì— ë”°ë¼ ë¬¸ì„œ ì „ì²´ê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# ë°ì´í„°ì— ìˆëŠ” ëª¨ë“  í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  # split() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ìƒ˜í”Œì„ í† í°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      # ë‹¨ì–´ë§ˆë‹¤ ê³ ìœ í•œ ì¸ë±ìŠ¤ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤, ì¸ë±ìŠ¤ 0ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "      token_index[word] = len(token_index)+1\n",
        "      \n",
        "# ìƒ˜í”Œì„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "# ê° ìƒ˜í”Œì—ì„œ max_length ê¹Œì§€ì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "max_length = 10\n",
        "\n",
        "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë°°ì—´ì…ë‹ˆë‹¤.\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
        "print(token_index)\n",
        "print(results.shape)\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i,j,index] = 1\n",
        "print(results)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
            "(2, 10, 11)\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3TruuFCTJ1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ë¬¸ì ìˆ˜ì¤€ì˜ ì›-í•« ì¸ì½”ë”©\n",
        "\n",
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable #ì¶œë ¥ ê°€ëŠ¥í•œ ëª¨ë“  ì•„ìŠ¤í‚¤(ASCII) ë¬¸ì\n",
        "token_index = dict(zip(characters, range(1, len(characters)+1)))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample[:max_length]):\n",
        "    index = token_index.get(character)\n",
        "    results[i,j,index] =1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH_ZLG02VJvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "f5ed730d-13be-4d0e-c56e-feab744dbf55"
      },
      "source": [
        "# ì¼€ë¼ìŠ¤ ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° ì´ìš© \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ 1,000ê°œì˜ ë‹¨ì–´ë§Œ ì„ íƒí•˜ë„ë¡ Tokenizer ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "tokenizer = Tokenizer(num_words = 1000)\n",
        "\n",
        "# ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# ë¬¸ìì—´ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "print(\"********ì •ìˆ˜ ì¸ë±ìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸*******\")\n",
        "print(sequences)\n",
        "\n",
        "\n",
        "# ì§ì ‘ ì›-í•« ì´ì§„ ë²¡í„° í‘œí˜„ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "print(\"********ì›-í•« ì´ì§„ ë²¡í„° í‘œí˜„*******\")\n",
        "print(one_hot_results)\n",
        "\n",
        "# ê³„ì‚°ëœ ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•\n",
        "word_index = tokenizer.word_index\n",
        "print(\"********ë‹¨ì–´ ì¸ë±ìŠ¤ì˜ ìˆ˜*******\")\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "********ì •ìˆ˜ ì¸ë±ìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸*******\n",
            "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
            "********ì›-í•« ì´ì§„ ë²¡í„° í‘œí˜„*******\n",
            "[[0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "********ë‹¨ì–´ ì¸ë±ìŠ¤ì˜ ìˆ˜*******\n",
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UprNQLoXXTNw",
        "colab_type": "text"
      },
      "source": [
        "<h5> ì›-í•« ì¸ì½”ë”©ì˜ ë³€ì¢… ì¤‘ í•˜ë‚˜ëŠ” ì›-í•« í•´ì‹± ê¸°ë²•ì…ë‹ˆë‹¤. ì´ ë°©ì‹ì€ ì–´íœ˜ ì‚¬ì „ì— ìˆëŠ” ê³ ìœ í•œ í† í°ì˜ ìˆ˜ê°€ ë„ˆë¬´ ì»¤ì„œ ëª¨ë‘ ë‹¤ë£¨ê¸° ì–´ë ¤ìš¸ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ë‹¨ì–´ì— ëª…ì‹œì ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ í• ë‹¹í•˜ê³  ì´ ì¸ë±ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ëŠ” ëŒ€ì‹  ë‹¨ì–´ë¥¼ í•´ì‹±í•˜ì—¬ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. \n",
        "  \n",
        "  ì´ ë°©ë²•ì˜ ì£¼ìš” ì¥ì ì€ ëª…ì‹œì ì¸ ë‹¨ì–´ ì¸ë±ìŠ¤ê°€ í•„ìš” ì—†ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  ì˜¨ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. => ì „ì²´ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì§€ ì•Šê³  í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ. \n",
        "  \n",
        "   ì£¼ì˜í•  ì ì€ ë‘ ê°œì˜ ë‹¨ì–´ê°€ ê°™ì€ í•´ì‹œë¥¼ ë§Œë“¤ì–´ ì¶©ëŒí•˜ëŠ” ê²½ìš°ì— ëª¨ë¸ì€ ë‹¨ì–´ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ í•´ì‹± ê³µê°„ì˜ ì°¨ì›ì„ í•´ì‹±ë  ê³ ìœ  í† í°ì˜ ì „ì²´ ê°œìˆ˜ë³´ë‹¤ í¬ê²Œí•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x8RjjQcWUhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# ë‹¨ì–´ë¥¼ í¬ê¸°ê°€ 1,000ì¸ ë²¡í„°ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "# 1,000ê°œ (ë˜ëŠ” ê·¸ ì´ìƒ)ì˜ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ í•´ì‹± ì¶©ëŒì´ ëŠ˜ì–´ë‚˜ê³  ì¸ì½”ë”©ì˜ ì •í™•ë„ê°€ ê°ì†Œë  ê²ƒì…ë‹ˆë‹¤. \n",
        "\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    # ë‹¨ì–´ë¥¼ í•´ì‹±í•˜ì—¬ 0ê³¼ 1,000 ì‚¬ì´ì˜ ëœë¤í•œ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i,j,index] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsRIXj_IgO6j",
        "colab_type": "text"
      },
      "source": [
        "<h5> ë‹¨ì–´ ì„ë² ë”© ì‚¬ìš©í•˜ê¸° </h5>\n",
        "ë‹¨ì–´ì™€ ë²¡í„°ë¥¼ ì—°ê´€ì§“ëŠ” ê°•ë ¥í•˜ê³  ì¸ê¸° ìˆëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ ë‹¨ì–´ ì„ë² ë”©ì´ë¼ëŠ” ë°€ì§‘ ë‹¨ì–´ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë§Œë“  ë²¡í„°ëŠ” í¬ì†Œí•˜ê³ (ëŒ€ë¶€ë¶„ 0ìœ¼ë¡œ ì±„ì›Œì§) ê³ ì°¨ì›ì…ë‹ˆë‹¤(ì–´íœ˜ ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ì˜ ìˆ˜ì™€ ì°¨ì›ì´ ê°™ìŠµë‹ˆë‹¤.). ë°˜ë©´ ë‹¨ì–´ ì„ë² ë”©ì€ ì €ì°¨ì›ì˜ ì‹¤ìˆ˜í˜• ë²¡í„°ì…ë‹ˆë‹¤. ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì–»ì€ ë‹¨ì–´ ë²¡í„°ì™€ ë‹¬ë¦¬ ë‹¨ì–´ ì„ë² ë”©ì€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµë©ë‹ˆë‹¤. \n",
        "\n",
        "ë³´í†µ 256ì°¨ì›, 512ì°¨ì› ë˜ëŠ” í° ì–´íœ˜ ì‚¬ì „ì„ ë‹¤ë£° ë•ŒëŠ” 1,024ì°¨ì›ì˜ ë‹¤ì–´ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°˜ë©´ ì›-í•« ì¸ì½”ë”©ì€ 20,000ê°œì˜ í† í°ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì–´íœ˜ ì‚¬ì „ì„ ë§Œë“¤ë ¤ë©´ 20,000ì°¨ì› ë˜ëŠ” ê·¸ ì´ìƒì˜ ë²¡í„°ì¼ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¨ì–´ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë©´ ë” ë§ì€ ì •ë³´ë¥¼ ì ì€ ì°¨ì›ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/8f1074670908ed80281db8c5feeca28d306f1bd8/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f626f6f6b2e6b657261732e696f2f696d672f6368362f776f72645f656d62656464696e67732e706e67\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUwonRmhdIB",
        "colab_type": "text"
      },
      "source": [
        "ë‹¨ì–´ ì„ë² ë”©ì„ ë§Œë“œëŠ” ë°©ë²•ì€ ë‘ ê°€ì§€ì…ë‹ˆë‹¤.\n",
        "\n",
        "- (ë¬¸ì„œ ë¶„ë¥˜ë‚˜ ê°ì„± ì˜ˆì¸¡ê³¼ ê°™ì€) ê´€ì‹¬ ëŒ€ìƒì¸ ë¬¸ì œì™€ í•¨ê»˜ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŸ° ê²½ìš°ì— ëœë¤í•œ ë‹¨ì–´ ë²¡í„°ë¡œ ì‹œì‘í•´ì„œ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "- í’€ë ¤ëŠ” ë¬¸ì œê°€ ì•„ë‹ˆê³  ë‹¤ë¥¸ ë¨¸ì‹  ëŸ¬ë‹ ì‘ì—…ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°ëœ ë‹¨ì–´ ì„ë² ë”©ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì´ë¥¼ ì‚¬ì „ í›ˆë ¨ëœ ë‹¨ì–´ ì„ë² ë”©ì´ë¼ê³  í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW92sJHwh4bK",
        "colab_type": "text"
      },
      "source": [
        "# Embeddingì¸µì„ ì‚¬ìš©í•´ ë‹¨ì–´ ì„ë² ë”© í•™ìŠµí•˜ê¸°\n",
        "ë‹¨ì–´ì™€ ë°€ì§‘ ë²¡í„°ë¥¼ ì—°ê´€ì§“ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ëœë¤í•˜ê²Œ ë²¡í„°ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì¸ë° ì´ëŸ° ë°©ì‹ì€ ì„ë² ë”© ê³µê°„ì´ êµ¬ì¡°ì ì´ì§€ ì•Šë‹¤ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì¦‰, accurateì™€ exactì´ë€ ë‹¨ì–´ëŠ” ëŒ€ë¶€ë¶„ì˜ ë¬¸ì¥ì—ì„œ ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì§€ë§Œ êµ¬ì¡°ì ì´ì§€ ëª»í•˜ë‹¤ë©´ ì™„ì „íˆ ë‹¤ë¥¸ ì„ë² ë”©ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. ì‹¬ì¸µ ì‹ ê²½ë§ì€ ì´ëŸ° êµ¬ì¡°ì ì´ì§€ ì•Šì€ ì„ë² ë”© ê³µê°„ì„ ì˜ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. \n",
        "\n",
        "ë‹¨ì–´ ë²¡í„° ì‚¬ì´ì—ì„œ ì„œë¡œ ì—°ê´€ëœ êµ¬ì¡°ë¥¼ ê°€ì§€ê¸° ìœ„í•´ì„œëŠ” ë‹¨ì–´ ì‚¬ì´ì— ìˆëŠ” ì˜ë¯¸ ê´€ê³„ë¥¼ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”©ì€ ì–¸ì–´ë¥¼ ê¸°í•˜í•™ì  ê³µê°„ì— ë§¤í•‘í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì˜ êµ¬ì¶•ëœ ì„ë² ë”© ê³µê°„ì—ì„œëŠ” ë™ì˜ì–´ê°€ ë¹„ìŠ·í•œ ë‹¨ì–´ ë²¡í„°ë¡œ ì„ë² ë”©ë  ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¼ë°˜ì ìœ¼ë¡œ ë‘ ë‹¨ì–´ ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì´ ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ ê±°ë¦¬ì™€ ê´€ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ë©€ë¦¬ ë–¨ì–´ì§ˆìˆ˜ë¡ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.) ë˜í•œ ê±°ë¦¬ ì™¸ì—ë„ ì„ë² ë”© ê³µê°„ì˜ íŠ¹ì • ë°©í–¥ë„ ì˜ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "ì‹¤ì œ ë‹¨ì–´ ì„ë² ë”© ê³µê°„ì—ì„œ ì˜ë¯¸ ìˆëŠ” ê¸°í•˜í•™ì  ë³€í™˜ì˜ ì˜ˆë¥¼ ë“¤ì–´ë³´ë©´ 'king' ë²¡í„°ì— 'female' ë²¡í„°ë¥¼ ë”í•˜ë©´ 'queen' ë²¡í„°ê°€ ë©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”© ê³µê°„ì€ ì „í˜•ì ìœ¼ë¡œ ì´ëŸ° í•´ì„ì´ ê°€ëŠ¥í•˜ê³  ì ì¬ì ìœ¼ë¡œ ìœ ìš©í•œ ìˆ˜ì²œ ê°œì˜ ë²¡í„°ë¥¼ íŠ¹ì„±ìœ¼ë¡œ ê°€ì§‘ë‹ˆë‹¤. \n",
        "\n",
        "ì‚¬ëŒì˜ ì–¸ì–´ë¥¼ ì™„ë²½í•˜ê²Œ ë§¤í•‘í•´ì„œ ì–´ë–¤ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ìƒì ì¸ ë‹¨ì–´ ì„ë² ë”© ê³µê°„ì´ ìˆì„ê¹Œìš”? ì•„ë§ˆë„ ê°€ëŠ¥í•˜ê² ì§€ë§Œ ì•„ì§ì€ ì´ëŸ° ì¢…ë¥˜ì˜ ê³µê°„ì„ ë§Œë“¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„¸ìƒì—ëŠ” ë§ì€ ë‹¤ë¥¸ ì–¸ì–´ê°€ ìˆê³  ì–¸ì–´ëŠ” íŠ¹ì • ë¬¸í™”ì™€ í™˜ê²½ì„ ë°˜ì˜í•˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.** ì‹¤ì œë¡œ ì¢‹ì€ ë‹¨ì–´ ì„ë² ë”© ê³µê°„ì„ ë§Œë“œëŠ” ê²ƒì€ ë¬¸ì œì— ë”°ë¼ í¬ê²Œ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ** ì˜ì–´ë¡œ ëœ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ìœ„í•œ ë‹¨ì–´ ì„ë² ë”© ê³µê°„ì€ ì˜ì–´ë¡œ ëœ ë²•ë¥  ë¬¸ì„œ ë¶„ë¥˜ ëª¨ë¸ì„ ìœ„í•œ ì„ë² ë”© ê³µê°„ê³¼ ë‹¤ë¥¼ ê²ë‹ˆë‹¤. ë”°ë¼ì„œ **ìƒˆë¡œìš´ ì‘ì—…ì—ëŠ” ìƒˆë¡œìš´ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ íƒ€ë‹¹í•©ë‹ˆë‹¤.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sbImTjQYiyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "512088e9-da39-4a5c-f25d-b7fe15a8eb95"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "# Embedding ì¸µì€ ì ì–´ë„ ë‘ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n",
        "# ê°€ëŠ¥í•œ í† í°ì˜ ê°œìˆ˜ (ì—¬ê¸°ì„œëŠ” 1,000ìœ¼ë¡œ ë‹¨ì–´ ì¸ë±ìŠ¤ ìµœëŒ“ê°’ +1 ì…ë‹ˆë‹¤)ì™€ ì„ë² ë”© ì°¨ì›(ì—¬ê¸°ì„œëŠ” 64)ì…ë‹ˆë‹¤.\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzfg3RVLU-Zp",
        "colab_type": "text"
      },
      "source": [
        "Embeddingì¸µì„ íŠ¹ì • ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë°€ì§‘ ë²¡í„°ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ì´í•´í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤. ì •ìˆ˜ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‚´ë¶€ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì´ ì •ìˆ˜ì— ì—°ê´€ëœ ë²¡í„°ë¥¼ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤. \n",
        "\n",
        "Embeddingì¸µì€ í¬ê¸°ê°€ (samples, sequence_length)ì¸ 2D ì •ìˆ˜ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ë°°ì¹˜ë“¤ì˜ ì‹œí€¸ìŠ¤ëŠ” ê¸¸ì´ê°€ ëª¨ë‘ ê°™ì•„ì•¼ í•˜ë¯€ë¡œ ì‘ì€ ê¸¸ì´ì˜ ì‹œí€¸ìŠ¤ëŠ” 0ìœ¼ë¡œ íŒ¨ë”©ë˜ê³  ê¸¸ì´ê°€ ë” ê¸´ ì‹œí€¸ìŠ¤ëŠ” ì˜ë¦½ë‹ˆë‹¤. => ëª¨ë“  ë¬¸ì¥ ê¸¸ì´ë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ ì§§ì€ ë¬¸ì¥ì˜ ë‚¨ì€ ë¶€ë¶„ì€ 0ìœ¼ë¡œ ì±„ì›Œì§€ê³ , ë„ˆë¬´ ê¸´ ê¸¸ì´ëŠ” ì˜ë¼ë²„ë¦°ë‹¤.\n",
        "\n",
        "Embedding ì¸µì˜ ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ê°€ì¤‘ì¹˜ëŠ” ë‹¤ë¥¸ ì¸µê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤. í›ˆë ¨í•˜ë©´ì„œ ì´ ë‹¨ì–´ ë²¡í„°ëŠ” ì—­ì „íŒŒë¥¼ í†µí•´ ì ì°¨ ì¡°ì •ë˜ì–´ ì´ì–´ì§€ëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„ë² ë”© ê³µê°„ì„ êµ¬ì„±í•©ë‹ˆë‹¤. í›ˆë ¨ì´ ëë‚˜ë©´ ì„ë² ë”© ê³µê°„ì€ íŠ¹ì • ë¬¸ì œì— íŠ¹í™”ëœ êµ¬ì¡°ë¥¼ ë§ì´ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3tRrSvBUqV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b6193768-9946-4927-88bf-b9d9e622feb4"
      },
      "source": [
        "# IMDB ì˜í™” ë¦¬ë·° ê°ì„± ì˜ˆì¸¡ ë¬¸ì œ \n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©í•  ë‹¨ì–´ì˜ ìˆ˜\n",
        "max_features = 10000\n",
        "\n",
        "# ì‚¬ìš©í•  í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ (ê°€ì¥ ë¹ˆë²ˆí•œ max_featuresê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©)\n",
        "maxlen = 20\n",
        "\n",
        "# ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œ\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "# ë¦¬ìŠ¤íŠ¸ë¥¼ (samples, maxlen) í¬ê¸°ì˜ 2D ì •ìˆ˜ í…ì„œë¡œ ë³€í™˜í•œë‹¤.\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5PjfXGOU6oH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "2713caff-80b1-4db6-b192-8f0011af4152"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding, Conv1D, Dropout, MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "# ë‚˜ì¤‘ì— ì„ë² ë”©ëœ ì…ë ¥ì„ Flatten ì¸µì—ì„œ í¼ì¹˜ê¸° ìœ„í•´ Embeddingì¸µì— input_lengthë¥¼ ì§€ì •\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "# Embedding ì¸µì˜ ì¶œë ¥ í¬ê¸°ëŠ” (samples, maxlen, 8)ì´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "# 3D ì„ë² ë”© í…ì„œë¥¼ (samples, maxlen * 8) í¬ê¸°ì˜ 2D í…ì„œë¡œ í¼ì¹©ë‹ˆë‹¤\n",
        "model.add(Flatten())\n",
        "\n",
        "# ë¶„ë¥˜ê¸°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                   epochs= 10,\n",
        "                   batch_size = 32,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 4s 186us/step - loss: 0.9465 - acc: 0.6184 - val_loss: 0.5666 - val_acc: 0.7186\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.5225 - acc: 0.7712 - val_loss: 0.6255 - val_acc: 0.7374\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.5095 - acc: 0.7977 - val_loss: 0.7013 - val_acc: 0.7358\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 3s 148us/step - loss: 0.4781 - acc: 0.8117 - val_loss: 0.7420 - val_acc: 0.7324\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 3s 151us/step - loss: 0.4504 - acc: 0.8246 - val_loss: 0.8657 - val_acc: 0.7160\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.4212 - acc: 0.8364 - val_loss: 0.9171 - val_acc: 0.7134\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 3s 152us/step - loss: 0.3842 - acc: 0.8448 - val_loss: 1.0277 - val_acc: 0.7034\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 3s 152us/step - loss: 0.3545 - acc: 0.8508 - val_loss: 1.1581 - val_acc: 0.6848\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.3259 - acc: 0.8505 - val_loss: 1.2770 - val_acc: 0.6746\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 3s 156us/step - loss: 0.3052 - acc: 0.8533 - val_loss: 1.4171 - val_acc: 0.6602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyb-T00pXadh",
        "colab_type": "text"
      },
      "source": [
        "ì•½ 75% ì •ë„ì˜ ê²€ì¦ ì •í™•ë„ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤. ë¦¬ë·°ì—ì„œ 20ë§Œê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•œ ê²ƒì¹˜ê³  ê½¤ ì¢‹ì€ ê²°ê³¼ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì„ë² ë”© ì‹œí€¸ìŠ¤ë¥¼ í¼ì¹˜ê³  í•˜ë‚˜ì˜ Denseì¸µì„ í›ˆë ¨í–ˆìœ¼ë¯€ë¡œ ì…ë ¥ ì‹œí€¸ìŠ¤ì— ìˆëŠ” ê° ë‹¨ì–´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì¦‰, ë‹¨ì–´ ì‚¬ì´ì˜ ê´€ê³„ë‚˜ ë¬¸ì¥ì˜ êµ¬ì¡°ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ê²ë‹ˆë‹¤. (ê° ì‹œí€¸ìŠ¤ ì „ì²´ë¥¼ ê³ ë ¤í•œ íŠ¹ì„±ì„ í•™ìŠµí•˜ë„ë¡ ì„ë² ë”© ì¸µ ìœ„ì— ìˆœí™˜ ì¸µì´ë‚˜ 1D í•©ì„±ê³± ì¸µì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.)"
      ]
    }
  ]
}