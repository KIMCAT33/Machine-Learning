{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리를 해보자",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KIMCAT33/Machine-Learning/blob/master/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%EB%A5%BC_%ED%95%B4%EB%B3%B4%EC%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UAdsu8MQljn",
        "colab_type": "text"
      },
      "source": [
        "<h2>자연어 처리를 해보자🤓</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mGsfOFHQ4pt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h5>예전에 모델이 학습하기 위해서 데이터를 모두 숫자로 바꿔야한다는 사실을 들었던 것이 기억나실겁니다.\n",
        "  \n",
        "  이를 위해 단어와 문자의 원-핫 인코딩을 통해 단어나 문자를 숫자로 바꿔보겠습니다.원-핫 인코딩은 토큰[단어/문자의 단위]을 벡터로 변환하는 가장 일반적이고 기본적인 방법입니다. 모든 단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i를 크기가 N(전체 어휘 종류의 수)인 이진 벡터[i번째 원소만 1이고 나머지는 모두 0인 벡터]로 변환합니다. </h5>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYgDap1ZQfFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "84deae86-57e3-4a89-ee34-01475a690e42"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 초기 데이터 : 각 원소가 샘플입니다.\n",
        "# 이 예에서는 하나의 샘플이 하나의 문장이지만 때에 따라 문서 전체가 될 수도 있습니다.\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# 데이터에 있는 모든 토큰의 인덱스를 구축합니다.\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  # split() 메서드를 사용해 샘플을 토큰으로 나눕니다.\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      # 단어마다 고유한 인덱스를 할당합니다, 인덱스 0은 사용하지 않습니다.\n",
        "      token_index[word] = len(token_index)+1\n",
        "      \n",
        "# 샘플을 벡터로 변환합니다.\n",
        "# 각 샘플에서 max_length 까지의 단어만 사용합니다.\n",
        "\n",
        "max_length = 10\n",
        "\n",
        "# 결과를 저장할 배열입니다.\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
        "print(token_index)\n",
        "print(results.shape)\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i,j,index] = 1\n",
        "print(results)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
            "(2, 10, 11)\n",
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3TruuFCTJ1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문자 수준의 원-핫 인코딩\n",
        "\n",
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable #출력 가능한 모든 아스키(ASCII) 문자\n",
        "token_index = dict(zip(characters, range(1, len(characters)+1)))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample[:max_length]):\n",
        "    index = token_index.get(character)\n",
        "    results[i,j,index] =1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH_ZLG02VJvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "f5ed730d-13be-4d0e-c56e-feab744dbf55"
      },
      "source": [
        "# 케라스 내부 유틸리티 이용 \n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만듭니다.\n",
        "tokenizer = Tokenizer(num_words = 1000)\n",
        "\n",
        "# 단어 인덱스를 구축합니다.\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# 문자열을 정수 인덱스의 리스트로 변환합니다.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "print(\"********정수 인덱스의 리스트*******\")\n",
        "print(sequences)\n",
        "\n",
        "\n",
        "# 직접 원-핫 이진 벡터 표현을 얻을 수 있습니다. \n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "print(\"********원-핫 이진 벡터 표현*******\")\n",
        "print(one_hot_results)\n",
        "\n",
        "# 계산된 단어 인덱스를 구하는 방법\n",
        "word_index = tokenizer.word_index\n",
        "print(\"********단어 인덱스의 수*******\")\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "********정수 인덱스의 리스트*******\n",
            "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
            "********원-핫 이진 벡터 표현*******\n",
            "[[0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "********단어 인덱스의 수*******\n",
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UprNQLoXXTNw",
        "colab_type": "text"
      },
      "source": [
        "<h5> 원-핫 인코딩의 변종 중 하나는 원-핫 해싱 기법입니다. 이 방식은 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용합니다. 각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신 단어를 해싱하여 고정된 크기의 벡터로 변환합니다. \n",
        "  \n",
        "  이 방법의 주요 장점은 명시적인 단어 인덱스가 필요 없기 때문에 메모리를 절약하고 온라인 방식으로 데이터를 인코딩할 수 있습니다. => 전체 데이터를 확인하지 않고 토큰을 생성할 수 있음. \n",
        "  \n",
        "   주의할 점은 두 개의 단어가 같은 해시를 만들어 충돌하는 경우에 모델은 단어 사이의 차이를 인식하지 못하는 문제가 발생하기 때문에 이를 방지하기 위해 해싱 공간의 차원을 해싱될 고유 토큰의 전체 개수보다 크게하는 방법이 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x8RjjQcWUhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# 단어를 크기가 1,000인 벡터로 저장합니다.\n",
        "# 1,000개 (또는 그 이상)의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소될 것입니다. \n",
        "\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    # 단어를 해싱하여 0과 1,000 사이의 랜덤한 정수 인덱스로 변환합니다.\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i,j,index] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsRIXj_IgO6j",
        "colab_type": "text"
      },
      "source": [
        "<h5> 단어 임베딩 사용하기 </h5>\n",
        "단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 단어 임베딩이라는 밀집 단어 벡터를 사용하는 것입니다. 원-핫 인코딩으로 만든 벡터는 희소하고(대부분 0으로 채워짐) 고차원입니다(어휘 사전에 있는 단어의 수와 차원이 같습니다.). 반면 단어 임베딩은 저차원의 실수형 벡터입니다. 원-핫 인코딩으로 얻은 단어 벡터와 달리 단어 임베딩은 데이터로부터 학습됩니다. \n",
        "\n",
        "보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원의 다어 임베딩을 사용합니다. 반면 원-핫 인코딩은 20,000개의 토큰으로 이루어진 어휘 사전을 만들려면 20,000차원 또는 그 이상의 벡터일 경우가 많습니다. 따라서 단어 임베딩을 사용하면 더 많은 정보를 적은 차원에 저장할 수 있습니다. \n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/8f1074670908ed80281db8c5feeca28d306f1bd8/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f626f6f6b2e6b657261732e696f2f696d672f6368362f776f72645f656d62656464696e67732e706e67\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUwonRmhdIB",
        "colab_type": "text"
      },
      "source": [
        "단어 임베딩을 만드는 방법은 두 가지입니다.\n",
        "\n",
        "- (문서 분류나 감성 예측과 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이런 경우에 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n",
        "\n",
        "- 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다. 이를 사전 훈련된 단어 임베딩이라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW92sJHwh4bK",
        "colab_type": "text"
      },
      "source": [
        "# Embedding층을 사용해 단어 임베딩 학습하기\n",
        "단어와 밀집 벡터를 연관짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것인데 이런 방식은 임베딩 공간이 구조적이지 않다는 문제점을 가지고 있습니다. 즉, accurate와 exact이란 단어는 대부분의 문장에서 비슷한 의미로 사용되지만 구조적이지 못하다면 완전히 다른 임베딩을 가지게 됩니다. 심층 신경망은 이런 구조적이지 않은 임베딩 공간을 잘 이해하지 못합니다. \n",
        "\n",
        "단어 벡터 사이에서 서로 연관된 구조를 가지기 위해서는 단어 사이에 있는 의미 관계를 반영해야 합니다. 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것입니다. 예를 들어 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩될 것입니다. 그리고 일반적으로 두 단어 벡터 사이의 거리는 이 단어 사이의 의미 거리와 관계되어 있습니다. (멀리 떨어질수록 다른 의미를 가지게 됩니다.) 또한 거리 외에도 임베딩 공간의 특정 방향도 의미를 가질 수 있습니다. \n",
        "\n",
        "실제 단어 임베딩 공간에서 의미 있는 기하학적 변환의 예를 들어보면 'king' 벡터에 'female' 벡터를 더하면 'queen' 벡터가 됩니다. 단어 임베딩 공간은 전형적으로 이런 해석이 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가집니다. \n",
        "\n",
        "사람의 언어를 완벽하게 매핑해서 어떤 자연어 처리 작업에도 사용할 수 있는 이상적인 단어 임베딩 공간이 있을까요? 아마도 가능하겠지만 아직은 이런 종류의 공간을 만들지 못했습니다. 세상에는 많은 다른 언어가 있고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않습니다.** 실제로 좋은 단어 임베딩 공간을 만드는 것은 문제에 따라 크게 달라집니다. ** 영어로 된 영화 리뷰 감성 분석 모델을 위한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 임베딩 공간과 다를 겁니다. 따라서 **새로운 작업에는 새로운 임베딩을 학습하는 것이 타당합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sbImTjQYiyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "512088e9-da39-4a5c-f25d-b7fe15a8eb95"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "# Embedding 층은 적어도 두 개의 매개변수를 받습니다.\n",
        "# 가능한 토큰의 개수 (여기서는 1,000으로 단어 인덱스 최댓값 +1 입니다)와 임베딩 차원(여기서는 64)입니다.\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzfg3RVLU-Zp",
        "colab_type": "text"
      },
      "source": [
        "Embedding층을 특정 단어를 나타내는 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해하는 것이 가장 좋습니다. 정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환합니다. \n",
        "\n",
        "Embedding층은 크기가 (samples, sequence_length)인 2D 정수 텐서를 입력으로 받습니다. 입력으로 들어가는 배치들의 시퀸스는 길이가 모두 같아야 하므로 작은 길이의 시퀸스는 0으로 패딩되고 길이가 더 긴 시퀸스는 잘립니다. => 모든 문장 길이를 통일하기 위해 짧은 문장의 남은 부분은 0으로 채워지고, 너무 긴 길이는 잘라버린다.\n",
        "\n",
        "Embedding 층의 객체를 생성할 때 가중치는 다른 층과 마찬가지로 랜덤하게 초기화됩니다. 훈련하면서 이 단어 벡터는 역전파를 통해 점차 조정되어 이어지는 모델이 사용할 수 있도록 임베딩 공간을 구성합니다. 훈련이 끝나면 임베딩 공간은 특정 문제에 특화된 구조를 많이 가지게 됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3tRrSvBUqV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b6193768-9946-4927-88bf-b9d9e622feb4"
      },
      "source": [
        "# IMDB 영화 리뷰 감성 예측 문제 \n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# 특성으로 사용할 단어의 수\n",
        "max_features = 10000\n",
        "\n",
        "# 사용할 텍스트의 길이 (가장 빈번한 max_features개의 단어만 사용)\n",
        "maxlen = 20\n",
        "\n",
        "# 정수 리스트로 데이터를 로드\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환한다.\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5PjfXGOU6oH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "2713caff-80b1-4db6-b192-8f0011af4152"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding, Conv1D, Dropout, MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding층에 input_length를 지정\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "# Embedding 층의 출력 크기는 (samples, maxlen, 8)이 됩니다.\n",
        "\n",
        "# 3D 임베딩 텐서를 (samples, maxlen * 8) 크기의 2D 텐서로 펼칩니다\n",
        "model.add(Flatten())\n",
        "\n",
        "# 분류기를 추가합니다.\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                   epochs= 10,\n",
        "                   batch_size = 32,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 4s 186us/step - loss: 0.9465 - acc: 0.6184 - val_loss: 0.5666 - val_acc: 0.7186\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.5225 - acc: 0.7712 - val_loss: 0.6255 - val_acc: 0.7374\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.5095 - acc: 0.7977 - val_loss: 0.7013 - val_acc: 0.7358\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 3s 148us/step - loss: 0.4781 - acc: 0.8117 - val_loss: 0.7420 - val_acc: 0.7324\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 3s 151us/step - loss: 0.4504 - acc: 0.8246 - val_loss: 0.8657 - val_acc: 0.7160\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.4212 - acc: 0.8364 - val_loss: 0.9171 - val_acc: 0.7134\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 3s 152us/step - loss: 0.3842 - acc: 0.8448 - val_loss: 1.0277 - val_acc: 0.7034\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 3s 152us/step - loss: 0.3545 - acc: 0.8508 - val_loss: 1.1581 - val_acc: 0.6848\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.3259 - acc: 0.8505 - val_loss: 1.2770 - val_acc: 0.6746\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 3s 156us/step - loss: 0.3052 - acc: 0.8533 - val_loss: 1.4171 - val_acc: 0.6602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyb-T00pXadh",
        "colab_type": "text"
      },
      "source": [
        "약 75% 정도의 검증 정확도가 나왔습니다. 리뷰에서 20만개의 단어만 사용한 것치고 꽤 좋은 결과입니다. 하지만 임베딩 시퀸스를 펼치고 하나의 Dense층을 훈련했으므로 입력 시퀸스에 있는 각 단어를 독립적으로 다루었습니다. 즉, 단어 사이의 관계나 문장의 구조를 고려하지 않았다는 겁니다. (각 시퀸스 전체를 고려한 특성을 학습하도록 임베딩 층 위에 순환 층이나 1D 합성곱 층을 추가하는 것이 좋습니다.)"
      ]
    }
  ]
}